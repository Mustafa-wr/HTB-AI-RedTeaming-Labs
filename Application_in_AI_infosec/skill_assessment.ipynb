{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression  # MUCH faster than GridSearch\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CPU-OPTIMIZED - FAST TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load data\n",
    "with open(\"skills_assessment_data/train.json\", \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "with open(\"skills_assessment_data/test.json\", \"r\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Keep sentiment words\n",
    "keep_words = {\"delighted\", \"moved\", \"uplifted\", \"breathtaking\", \"soared\", \"empty\", \"annoyed\", \"not\", \"no\", \"barely\"}\n",
    "stop_words = stop_words - keep_words\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"[^a-z\\s$!']\", \"\", text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "print(\"Preprocessing (fast)...\")\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(preprocess_text)\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    min_df=2,           # Faster\n",
    "    max_df=0.85, \n",
    "    ngram_range=(1, 2), # Only bigrams (trigrams too slow)\n",
    "    max_features=10000  # Limit features for CPU\n",
    ")\n",
    "\n",
    "# Use LogisticRegression (100x faster than MultinomialNB with GridSearch)\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorizer\", vectorizer),\n",
    "    (\"classifier\", LogisticRegression(C=2.0, max_iter=500, solver='saga', random_state=42))\n",
    "])\n",
    "\n",
    "print(\"Training (30-60 seconds on CPU)...\")\n",
    "pipeline.fit(train_df[\"text\"], train_df[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(test_df[\"text\"])\n",
    "accuracy = accuracy_score(test_df[\"label\"], predictions)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ ACCURACY: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "if accuracy >= 0.90:\n",
    "    print(\"âœ… 90% ACHIEVED!\")\n",
    "else:\n",
    "    print(f\"âš  {(0.90-accuracy)*100:.2f}% short\")\n",
    "\n",
    "print(\"\\n\" + classification_report(test_df[\"label\"], predictions))\n",
    "\n",
    "# Save\n",
    "joblib.dump(pipeline, 'skills_assessment.joblib')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
